{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import usefull packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from auchan_scraper.database import create_table, select_all\n",
    "from auchan_scraper.get_categories import get_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating tables we can start the search for all categories urls. For debugging purposes we use logging package. While scraping the urls of each subcategory, it is also saving the cookies for each, as without this process we would get response 400. After the process both categories and cookies are saved in json files (for future use in crawler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://zakupy.auchan.pl/shop/artykuly-spozywcze.c-11908\" # URL of the food categories page\n",
    "categories, cookies = get_categories(url)\n",
    "\n",
    "with open(\"auchan_scraper/categories.json\", \"w\") as categories_file:\n",
    "    json.dump(categories, categories_file)\n",
    "\n",
    "with open(\"auchan_scraper/cookies.json\", \"w\") as cookies_file:\n",
    "    json.dump(cookies, cookies_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preview cookies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"auchan_scraper/cookies.json\", \"r\") as cookies_file:\n",
    "    cookies = json.load(cookies_file)\n",
    "for cookie in cookies:\n",
    "    for key in cookie:\n",
    "        print(key, cookie[key], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## count number of subcategories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('auchan_scraper/categories.json', 'r') as f:\n",
    "    categories = json.load(f)\n",
    "\n",
    "url_count = 0\n",
    "for category in categories:\n",
    "    for subcategory in category['subcategories']:\n",
    "        if 'url' in subcategory:\n",
    "            url_count += 1\n",
    "print(url_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import spider to run without command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auchan_scraper.spiders.shop import ShopSpider\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.utils.project import get_project_settings\n",
    "import nest_asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ShopSpider we used following settings:\n",
    "- ROBOTSTXT_OBEY = True (obey the robots.txt file: get response 200)\n",
    "- DOWNLOAD_DELAY = 3 (3 second delay between each fetch of the data)\n",
    "- COOKIES_ENABLED = True (without cookies user is not able to acces the api fetching data from the server)\n",
    "- REQUEST_FINGERPRINTER_IMPLEMENTATION = \"2.7\"\n",
    "- TWISTED_REACTOR = \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n",
    "- FEED_EXPORT_ENCODING = \"utf-8\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also used additional middlewares to rotate headers:\n",
    "- 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware' : None,\n",
    "- 'scrapy_user_agents.middlewares.RandomUserAgentMiddleware' : 400\n",
    "\n",
    "And custom pipelines:\n",
    "- \"auchan_scraper.pipelines.DuplicatesPipeline\": 100, (removes duplicated items)\n",
    "- \"auchan_scraper.pipelines.SavingTosqlitePipeline\": 200 (saves yield data to SQLite db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run scrapy in jupyter notebook in order to encounter error: \"RuntimeError: This event loop is already running\" we can use asyncio to create a event loop that can coexist with the one being used by Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "process = CrawlerProcess(get_project_settings())\n",
    "\n",
    "process.crawl(ShopSpider, number=3)\n",
    "\n",
    "process.start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can select all saved items from the DB and make a pandas dataFrame from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = select_all()\n",
    "columns = [\"product_id\", \"product_name\", \"category_name\", \"price\", \"currency\", \"volume\", \"unit\", \"volume_info\", \"package_unit\", \"package_size\"]\n",
    "\n",
    "df = pd.DataFrame(rows, columns=columns)\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
